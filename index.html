<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Point Pattern Analysis</title>
    <meta charset="utf-8" />
    <meta name="author" content="Martin Hinz" />
    <script src="libs/header-attrs-2.6/header-attrs.js"></script>
    <link rel="stylesheet" href="libs/default.css" type="text/css" />
    <link rel="stylesheet" href="libs/default-fonts.css" type="text/css" />
    <link rel="stylesheet" href="libs/customize.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">

class: title-slide, center, middle


#  Point Pattern Analysis

##  Principles, Methods and Application in R

###  Martin Hinz

####  Institut für Archäologische Wissenschaften, Universität Bern

18.03.2021

.footnote[
.right[
.tiny[
You can find the code and data of this presentation at [https://github.com/MartinHinz/pppa_brno_2021](https://github.com/MartinHinz/pppa_brno_2021).

For this presentation, intensive use was made of the online book "Intro to GIS and Spatial Analysis". The book can be found at the following address and contains other exciting things: [https://mgimond.github.io/Spatial](https://mgimond.github.io/Spatial)
]
]
]

---
class: inverse, bottom, right
# Introduction and Setting the Frame

---
## Definition

&gt; *Point pattern analysis (PPA) is the study of the spatial arrangements of points in (usually 2-dimensional) space. - wikipedia*

### Centrography
- Measuring the parameters of some point distribution

### Density Analysis
- Visualisation ended up rotation of the density of points

### Distance based Analysis
- Statistical analyses of the distance between points and its distribution

### Modeling
- Modelling spatial processes and comparing the results to actual observed data

---

## Difficulties and tasks

### Get the data
You are responsible.

### Get the data into R
You will see how data needs to be structured to be able to read it into R.

### Using the right tools in the right way
I will demonstrate some tools and their application in special statistics.

### Making the right interpretations and conclusions
You are responsible (again).

---
## Install some packages

You need:


```r
list.of.packages &lt;- c("sf", # Handling spatial data
                      "raster", # Handling raster data
                      "maptools", # additional spatial functionality
                      "spatstat", # spatial statistic 
                      "ggplot2", # general plotting library
                      "rnaturalearth" # Getting some Geodata
)

new.packages &lt;- list.of.packages[
  !(list.of.packages %in% installed.packages()[,"Package"])
  ]

if(length(new.packages)) install.packages(new.packages)
```

Optional:


```r
# package for centrographic statistics
# We only might need it once
if (!require("aspace")) install.packages("aspace")
```


---
class: inverse, bottom, right
# Data Preparation and Import

---

## Get some data

* [location of 14C dated sites in Czech Republic](data/c14sites.csv)
* [location of 14C dated sites in Czech Republic in a different format](data/c14sites_comma.csv)

Save them to a folder of your choice

### Set your working directory

The easiest at the moment in R at Menu &gt; Session &gt; Set Working Directory &gt; Choose Directory

(there are other ways...)

Point to the same folder where you saved the files

---

## Import formats

### Probably the standard (and worst possible) starting point

- Spreadsheet data 
  - here from our 14C database in development XRONOS
  - sites with 14C data of Czech Republic
  - not very complete... you probably know better sources (for the time being ;-) )

![](images/example_sources.png)
---

### Save as CSV

- CSV: Comma Separated Values
- you can im-/export csv from every spreadsheet software (MS Excel, Libre Office Calc, ...)

![](images/csv_save_excel_libre.png)

---
### CSV vs. 'CSV2'

- two flavours:
  - Anglo-American: comma ',' as column separator, point '.' as decimal separator
  - 'continental': semicolon as column separator, comma ',' as decimal separator
- which one you use depends on your language settings

![](images/csv_differences.png)

---

## Reading csv into R

For reading CSV data into R, you can use the 'read.csv()' function.

Pay attention to use the correct function for your language settings.


```r
 # if you have an anglo-american csv (comma)
data &lt;- read.csv("data/c14sites_comma.csv")
# if you have a continental csv (semicolon)
data &lt;- read.csv2("data/c14sites.csv") 

data
```

```
##                         site     lat     lng
## 1                    Hlinsko 49.7629 15.9095
## 2                 Kutná Hora 49.9474 15.2674
## 3  Mnichovo Hradiště, Dneboh 50.5307 15.0335
## 4     Mohelnice, U cukrovaru 49.7833 16.9167
## 5                    Olomouc 49.5767 17.2311
## 6                  Vedrovice 49.0217 16.3808
## 7              Velké Hoštice 49.9300 17.9600
## 8             Blucina Cezavy 49.0400 16.6200
## 9                     Bylany 49.6667 15.2500
## 10                    Bezděz 50.5342 14.7200
## 11                  Chudeřín 50.3372 13.4470
## 12           Mušov Neurissen 48.9100 16.5640
## 13          Mušov Königsgrab 48.9000 16.5611
## 14              Okrouhlík I. 50.8420 14.3550
## 15          Uhelná rokle II. 50.6053 14.6846
## 16       Orlík-Krkavčí skála 49.5108 14.1653
## 17          Pod Černou Louží 49.5100 15.5130
## 18            Radovesice III 50.4110 14.0683
## 19                   Ďáblice 50.1423 14.4767
## 20             Jenišův Újezd 50.5667 13.7167
## 21            Jezevčí převis 50.7818 14.2145
## 22      Kostelec nad Vltavou 49.5012 14.2109
## 23                    Měšice 50.1969 14.5194
## 24 Radkovice-Osobovská skála 49.4796 13.4580
## 25  Sutny, Těšetice-Kyjovice 48.8960 16.1360
## 26                 Prasklice 49.2688 17.1858
## 27              Slanska Hora 50.2340 14.1000
## 28                   Blučina 49.0550 16.6465
## 29  Miškovice; site: Praha 9 50.1579 14.5454
## 30                 Dluhonice 49.4544 17.4128
## 31               Postoloprty 50.3598 13.7029
## 32                     Becov 50.4339 13.7233
## 33        Zadní Hon, Březník 49.1716 16.1945
## 34             Brno - Bystrc 49.2266 16.5311
## 35                Brno-Lisen 49.2082 16.6947
## 36       Brodek u Prostějova 49.3701 17.0907
## 37                 Vikletice 50.3500 13.4025
## 38                  Dobešice 49.3268 14.2602
## 39                   Homolka 49.0308 14.9891
## 40                  Denemark 49.9277 15.2528
## 41                  Lovosice 50.5146 14.0515
## 42                 Makotřasy 50.1445 14.2147
## 43               Máselník I. 50.6866 14.5370
## 44               Nový Bydžov 50.2415 15.4908
## 45                  Nemilany 49.5585 17.2463
## 46                    Řepčín 49.6079 17.2343
## 47                  Slavonin 49.5700 17.2300
## 48                    Pavlov 48.8742 16.6723
## 49         Pavlov-Horní Pole 48.8745 16.6711
## 50               Pod křídlem 50.6670 14.5328
## 51                     Slany 50.2269 14.0837
## 52  Vlkov u Spáleného-Babiny 49.6029 13.5771
## 53                 Spytihněv 49.1405 17.4982
## 54                  Tušimice 50.3833 13.3667
## 55                 Tvořihráz 48.9172 16.1360
## 56                      Žopy 49.3326 17.6094
## 57                  Heřmánky 49.7069 17.7678
## 58      Otmíče-Otmíčská hora 49.8633 13.9480
## 59               Šídelník I. 50.5700 14.5200
## 60                    Toušeň 50.1695 14.7158
## 61      Mířkov-Racovský vrch 49.5898 12.8798
## 62               Vysoká Lípa 50.8560 14.3480
## 63   Starý zámek, Suchohrdly 48.8683 16.0948
## 64             Na Kocourkách 49.0287 16.4376
## 65                Slavonín 1 49.5691 17.2348
## 66                Přáslavice 49.5847 17.3914
## 67          Brno - Ivanovice 49.2641 16.5657
## 68               Černá Louže 50.8556 14.3503
## 69               Chabařovice 50.6732 13.9418
## 70                  Holubice 49.1775 16.8121
## 71                     Hulín 49.3171 17.4639
## 72                 Budkovice 49.0725 16.3462
## 73                  Těšetice 48.8890 16.1581
## 74                     Mokrá 49.2233 16.7516
## 75              Prag 6, Baba 50.0500 14.4167
## 76                    Donbas 49.4180 17.3108
## 77         Šebkovice, Třebíč 49.1206 15.8198
## 78           Široké Třebčice 50.2807 13.3835
## 79                    Zalany 50.5833 13.9000
```

---

## From data to spatial data

- There are multiple packages for handling spatial data
- Which one to use depends on preferences
- We are using sf here


```r
library(sf)
```

```
## Linking to GEOS 3.8.1, GDAL 3.1.4, PROJ 6.3.1
```

```r
# Convert the dataframe to a spatial object. Note that the
# crs= 4326 parameter assigns a WGS84 coordinate system to the 
# spatial object
data.sf &lt;- st_as_sf(data,
                    coords = c("lng", "lat"), # coordinate columns
                    crs = 4326)  # coordinate references system in epsg
data.sf
```

```
## Simple feature collection with 79 features and 1 field
## geometry type:  POINT
## dimension:      XY
## bbox:           xmin: 12.8798 ymin: 48.8683 xmax: 17.96 ymax: 50.856
## geographic CRS: WGS 84
## First 10 features:
##                         site                geometry
## 1                    Hlinsko POINT (15.9095 49.7629)
## 2                 Kutná Hora POINT (15.2674 49.9474)
## 3  Mnichovo Hradiště, Dneboh POINT (15.0335 50.5307)
## 4     Mohelnice, U cukrovaru POINT (16.9167 49.7833)
## 5                    Olomouc POINT (17.2311 49.5767)
## 6                  Vedrovice POINT (16.3808 49.0217)
## 7              Velké Hoštice     POINT (17.96 49.93)
## 8             Blucina Cezavy     POINT (16.62 49.04)
## 9                     Bylany   POINT (15.25 49.6667)
## 10                    Bezděz   POINT (14.72 50.5342)
```

---

## Mapping the data

.pull-left[
### simple

Just use plot


```r
plot(data.sf) # inbuild plots
```

![](index_files/figure-html/unnamed-chunk-11-1.png)&lt;!-- --&gt;
]

.pull-right[
### more elaborated

using the gglot and natural earth data

.tiny[

```r
library(ggplot2) # for nicer plots
library(rnaturalearth) # for geodata

worldmap &lt;- ne_countries(scale = 'medium',
                         type = 'map_units',
                         returnclass = 'sf')
ggplot() + geom_sf(data = worldmap) + geom_sf(data=data.sf) +
    coord_sf(xlim = c(0,40), ylim = c(45,55)) + theme_bw()
```

![](index_files/figure-html/unnamed-chunk-12-1.png)&lt;!-- --&gt;
]]


---

## Problem: lng-lat vs. projection

- Most of the tools for point pattern analyses required distances
- Unprojected coordinate reference systems like the longitude latitude system are not distance correct
- We have to re-project our data

.pull-left[

```r
plot(data.sf, graticule = TRUE)
```

![](index_files/figure-html/unnamed-chunk-13-1.png)&lt;!-- --&gt;
]

.pull-right[

```r
data.utm &lt;- st_transform(data.sf,
                         crs = 32633)
plot(data.utm, graticule = TRUE)
```

![](index_files/figure-html/unnamed-chunk-14-1.png)&lt;!-- --&gt;
]

---

## To come from lng/lat spreadsheat to R spatial data:

### in three simple steps


```r
data &lt;- read.csv2("data/c14sites.csv")
data.sf &lt;- st_as_sf(data,
                    coords = c("lng", "lat"), # coordinate columns
                    crs = 4326)  # coordinate references system in epsg
data.utm &lt;- st_transform(data.sf, crs = 32633)
```

---
class: inverse, bottom, right
# Centrography

---

## Center, standard distance and standard deviational ellipse
.pull-left[
![](images/centrography.svg)
.caption[source: https://mgimond.github.io]
]

.pull-left[
.tiny[

```r
mean_center &lt;- apply(st_coordinates(data.utm),
                     2,
                     mean)

mean_center.df &lt;- as.data.frame(t(mean_center))

mean_center.utm &lt;- st_as_sf(mean_center.df,
                    coords = c("X", "Y"), 
                    crs = 32633)

standard_distance &lt;- mean(
  st_distance(mean_center.utm,
              data.utm))

dx &lt;- sqrt(
  sum(
    (st_coordinates(data.utm)[,1]-
         mean_center[1])^2
    ) / nrow(data.utm)
)

dy &lt;- sqrt(
  sum(
    (st_coordinates(data.utm)[,2]-
       mean_center[2])^2
    ) / nrow(data.utm)
)
```
]
]

---

## Plot Center, standard distance and standard deviational ellipse
.pull-left[
![](index_files/figure-html/unnamed-chunk-17-1.png)&lt;!-- --&gt;
]

.pull-left[
.tiny[

```r
# you do not have to run this: installing and loading
# aspace requires a lot of dependencies!
library(aspace)

# calculating the ellipse
ellipse &lt;- st_as_sf(
  calc_sde(points = st_coordinates(data.utm)),
  coords = c(2,3), 
  crs = 32633)

# Ploting everything
ggplot() + geom_sf(data=worldmap) + geom_sf(data=data.utm) +
  geom_sf(data=mean_center.utm, color="red") +
  geom_sf(data=st_buffer(mean_center.utm, standard_distance),
          fill="red", alpha = 0.5) +
  geom_sf(data=ellipse, fill="yellow", alpha = 0.5) +
    coord_sf(xlim = c(10,20), ylim = c(48,52)) + theme_bw() 
```
]
]
---
class: inverse, bottom, right
# Density based analysis

---
## Global density

The Density of a pattern in relationship to the overall area.

$$
\widehat{\lambda} = \frac{n}{a}
\label{eq:global-density}
$$


```r
# get the geodata of Czech Republic
cz.lnglat &lt;- ne_countries(country = 'Czech Republic',
                          returnclass = "sf", scale = "medium")

# transform from Lat/Lng WGS84 to UTM
cz &lt;- st_transform(cz.lnglat, crs = 32633)

# calculate the area
cz.area &lt;- st_area(cz)

# applying the formula
nrow(data.utm)/cz.area
```

```
## 1.003729e-09 [1/m^2]
```

---
## Local density

The same, but measured at different locations, not only once for all. Which location depends on the approach.

.pull-left[
### Preparations

At first we have to make our dataset available for spatstat. For this, we have to transform it into a point pattern data.


```r
library(spatstat)

# Transform to ppp
data.ppp &lt;- as.ppp(data.utm)
# 'Unmark' the data, removing names
marks(data.ppp)  &lt;- NULL 
# setting working area to CZ
Window(data.ppp) &lt;- as.owin(cz) 

plot(data.ppp)
```
]

.pull-right[
![](index_files/figure-html/unnamed-chunk-21-1.png)&lt;!-- --&gt;
]
---

### Quadrat density

Divide the area into squares and calculate the density by square.

.pull-left[
.tiny[

```r
data.quad &lt;- quadratcount(data.ppp,  # Count per Square
                        nx= 8, ny=4) # Divided into 8x4

plot(data.quad)
```

![](index_files/figure-html/unnamed-chunk-22-1.png)&lt;!-- --&gt;
]
]

.pull-right[
.tiny[

```r
plot(
  intensity(data.quad,    # Calculate count/area
               image = T) # and return as map
  )
```

![](index_files/figure-html/unnamed-chunk-23-1.png)&lt;!-- --&gt;
]
]

---

### Kernel density

.pull-left[
With a kernel density estimation we avoid the problems of unequal base sizes resulting from the square. We also avoid arbitrary borders within our investigation area.


```r
data.kde &lt;- density(data.ppp)
plot(data.kde)
contour(data.kde, add=T)
```

![](index_files/figure-html/unnamed-chunk-24-1.png)&lt;!-- --&gt;
]

.pull-right[
Different kernel sizes can influence the result of the interpolation.


```r
# kernel is 10 km
data.kde2 &lt;- density(data.ppp,
                     sigma = 10000)
plot(data.kde2)
contour(data.kde2, add=T)
```

![](index_files/figure-html/unnamed-chunk-25-1.png)&lt;!-- --&gt;
]

---
class: inverse, bottom, right
# Distance based analysis

---
## Density vs. Distance

- Investigating how points are distributed in respect to each other (second order property) vs. distributed in respect to the study extend
- reveals more of the mechanics of the process than its relationship to the general space and its features

.pull-left[
![](index_files/figure-html/unnamed-chunk-26-1.png)&lt;!-- --&gt;
]

.pull-right[
![](index_files/figure-html/unnamed-chunk-27-1.png)&lt;!-- --&gt;
]

---
# Density (Point Pattern) Analysis
## Motivation

.pull-left[
* most basically tries to answer the question if data are clustered, regular spaced or random
* uninfluenced spatial processes tend to produce random patterns
* [Complete Spatial Randomness](https://en.wikipedia.org/wiki/Complete_spatial_randomness)
]

.pull-right[
![](images/AM07_Fig3.png)
]
.caption[https://gistbok.ucgis.org/]

---

## Archaeological Motivation

.pull-left[
* clustered patterns result from 'attractive' processes
  * if the presence of one object makes other objects more **likely**
  * eg. burial sites: usually, we have burial grounds/areas, so that burials tend to be clustered in the landscape
* regular patterns result from 'repulsive' processes
  * if the presence of one object makes other objects more **unlikely**
  * eg. settlement sites: usually, settlements need some area/hinterland, so that settlements tend to be more regular in the landscape
]

.pull-right[
![](images/AM07_Fig3.png)
]
.caption[https://gistbok.ucgis.org/]
---

## Nearest Neighbor

How far, for every point, is the next (first, but sometimes also second, third) nearest neighbouring point.

The distribution of the Nearest Neighbours differ for different spatial distributions! Simulation:

.pull-left[
![](index_files/figure-html/unnamed-chunk-28-1.png)&lt;!-- --&gt;
]

.pull-right[
![](index_files/figure-html/unnamed-chunk-29-1.png)&lt;!-- --&gt;
]

---
## The code for that example

.pull-left[
Simulate and plot

.tiny[

```r
# Making an study area of 10x10 units
win &lt;- owin(c(0,10),c(0,10)) 

# Fixing the random number generator
set.seed(12)

# Random coordinates, centered
x &lt;- rnorm(20, 5,1)
y &lt;- rnorm(20,5,1)

# our clustered ppp
P.cl &lt;- ppp(x,y,window=win)

# Regular ppp using an inhibition process (repulsive)
P.reg &lt;- rSSI(2, 20, win = win)

# A CSR ppp
P.rnd &lt;- rpoint(20, win=win)
```
]]

.pull-right[
Plot

.tiny[

```r
# Define plots side by side and plot
OP &lt;- par(mfrow=c(1,3), mar=c(1,1,1,0))
 plot(P.reg, pch=20, cols=rgb(0,0,0,.5),
      main = "Regular Pattern")
 plot(P.cl, pch=20, cols=rgb(0,0,0,.5), main =
        "Clustered Pattern")
 plot(P.rnd, pch=20, cols=rgb(0,0,0,.5), main =
        "Random Pattern")
par(OP)
```
]
Histogram

.tiny[

```r
# Define plots side by side and make histograms
OP &lt;- par(mfrow=c(1,3), mar=c(1,1,1,0))
hist(nndist(P.reg), xlim=c(0,4), ylim=c(0,20),
     main = "Regular Pattern")
hist(nndist(P.cl), xlim=c(0,4), ylim=c(0,20),
     main = "Clustered Pattern")
hist(nndist(P.rnd), xlim=c(0,4), ylim=c(0,20),
     main = "Random Pattern")
par(OP)
```
]]

---

## Average Nearest Neighbor (ANN)

.pull-left[
**Average Nearest Neighbor (ANN)** measures the average distance of neighboring data points from a given observation. If it is compared with a theoretical random average distance, it tells much about whether data points are clustered or dispersed.
]

.pull-right[
![](images/1_ANfLax8UZKlHqyCWK3uR1A.png)
.caption[Average distance of nearest neighbors are shorter in the left. Source: https://towardsdatascience.com]
]



`\(NearestNeighbor Dist ance = d(NN) = \frac{\sum^n_{i=1} min(d_{ij})}{N}\)`
`\(Theoretical Random Distance = d(ran) = 0.5*\sqrt\frac{A}{N}\)`
`\(Nearest Neighbor Index = NNI = \frac{d(NN)}{d(ran)}\)`

---

## Average Nearest Neighbor (ANN)

.pull-left[
`\(Nearest Neighbor Index = NNI = \frac{d(NN)}{d(ran)}\)`

* if the index is **below** 1, the points tend to be clustered
* if the index is **above** 1, the points tend to be regular
* if the index is **around** 1, the points tend to be random
]

.pull-right[
![](images/NNA.png)
.caption[Average distance of nearest neighbors are shorter in the left. Source: https://towardsdatascience.com]
]

---
## Average Nearest Neighbor (ANN) in R

A simple measurement for the average value of the nearest neighbour of a point pattern 

.pull-left[
![](index_files/figure-html/unnamed-chunk-33-1.png)&lt;!-- --&gt;
]

.pull-right[

```r
mean(nndist(P.reg))
```

```
## [1] 2.119471
```

```r
mean(nndist(P.cl))
```

```
## [1] 0.4333607
```

```r
mean(nndist(P.rnd))
```

```
## [1] 1.180716
```

```r
ann.ran &lt;- 0.5 * sqrt(10*10 / 20)
ann.ran
```

```
## [1] 1.118034
```
]

---
## Average Nearest Neighbor (ANN) for CZ

.pull-left[

```r
data.ann &lt;- mean(nndist(data.ppp))
data.ann
```

```
## [1] 10613.68
```

```r
a_data &lt;- area(as.owin(cz))
n_data &lt;- nrow(data.utm)
ann.ran &lt;- 0.5 * sqrt(a_data/n_data)

data.ann/ann.ran
```

```
## [1] 0.6725187
```
]

.pull-right[
![](images/NNA.png)
.caption[Average distance of nearest neighbors are shorter in the left. Source: https://towardsdatascience.com]
]
---

## Nearest Neighbor Analysis - Restrictions

.pull-left[
- what we consider regular is strongly scale dependend
- also, some processes might be regular in one scale, while clustered in another
- eg. burials around settlements:
  - Settlements might be regular spaced
  - Burials might be clustered around settlements
  - if we have only the burials, in the landscape the might have a similar structure like the example
]

.pull-right[
![](images/NNA05.png)
]

---

## Neighbor Analysis over multiple scales
### The K function

.pull-left[
The **K-Function** (or **Ripley`s K**) measures distances between all points in space rather than just the neighbors as in ANN. It also helps to understand how clustering or dispersion of data occurs in different distances from its center (the centroid).

![:width 60%](images/k_function02.png)
.caption[Statistical significance of Ripley’s K function. Source: https://gistbok.ucgis.org]
]
.pull-right[
![](images/k_function01.png)
.caption[K-Function describing how clustering occurs in different scanning horizon from the center. Source: https://towardsdatascience.com]
]

---

### K Function in R

.pull-left[

```r
K &lt;- Kest(data.ppp)
plot(K)
```

![](index_files/figure-html/unnamed-chunk-36-1.png)&lt;!-- --&gt;
]
.pull-right[
Ripley`s K with different corrections for edge effects:

* black: isotropic correction, to be used for regular windows
* red: translation correction, to be used for arbitratry masks (like in our case)
* blue: 'border method' correction, fast, but inaccurated. Best to be ignored

additionally:

* blue: the theoretical random distribution

**Interpretation**: If the red line is above the blue line, the points are clustered, if it is below, they are dispersed (regular distributed)
]
---

### K Funktion with Envelope
.pull-left[

```r
plot(envelope(data.ppp, Kest))
```
![](index_files/figure-html/unnamed-chunk-38-1.png)&lt;!-- --&gt;
]

.pull-right[
**When does random become clustered?**

Approach: try out different random pattern, and mark there extremes. We produce an *Envelope* by default of 2 standard deviations (96% of the cases in normal distributed cases).
* If the actual data are below: than dispersed
* If the actual data are above: than clustered

**-&gt; Monte Carlo Approach**

*Benefits*:
* we see a range instead of a point estimate of random processes
* it comes with battery included: correction for edge effects takes place automatically

*Drawbacks*:
* the 'confidence intervals' are random, will change with every run
* accuracy depends on number of simulations
]
---

## inhomogenous point processes

.pull-left[
The ordinary K function is only defined for 

* **homogeneous point patterns (resulting from stationary point pattern):** The basic density of the points is the same over the whole investigation area

-&gt; This assumption is in real world hard to maintain

* **inhomogeneous point patterns (resulting from non-stationary point pattern):** The density differs over the study area according some spatial trends

]

.pull-right[
![](images/AnIntroductiontoStationaryandNon-StationaryProcesses1_4-babcf20c2229411f8da42e24e7aaa18f.png)
.caption[Image by Julie Bang © Investopedia 2020.]
]
---
## Trend in the real world data

.pull-left[

In our data there is an obvious trend: Dense in the northwest and the southeast.


```r
lambda &lt;- density.ppp(data.ppp)
plot(lambda, main="", ribbon = NA)
plot(data.ppp, add=T)
```

Plotting distances against coordinates.


```r
plot(data.ppp$x,nndist(data.ppp))
```

This likely results from other factors than the point process: **Covariates**.

That is why the homogenuous assumption is not met!

]

.pull-right[
&lt;img src="index_files/figure-html/unnamed-chunk-41-1.png" width="75%" /&gt;

&lt;img src="index_files/figure-html/unnamed-chunk-42-1.png" width="85%" /&gt;
]
---
## inhomogenous point processes in R

.pull-left[
Introducing Kinhom, the Inhomogeneous K-function.

* Estimates the inhomogeneous K function of a non-stationary point pattern
* Needs lambda: An estimation how likely it is for points to occure in different areas
* If omitted, it will be estimated from the data
]

.pull-right[

```r
K_in &lt;- Kinhom(data.ppp,
               lambda = lambda) 

plot(K_in)
```

![](index_files/figure-html/unnamed-chunk-43-1.png)&lt;!-- --&gt;
]

---
## inhomogenous point processes in R with envelope

.pull-left[
Also, for Kinhom an envelope is possible.

* explicite parameterisation of the simulation function necessary
  * a Poisson Point Pattern
  * based on the intensity lambda (our density map)
* explicit parameterisation of the K function

-&gt; Exept for the repetition, we already defined our own Monte Carlo procedure. Kind of...
]

.pull-right[
.tiny[

```r
Ken &lt;- envelope(data.ppp, 'Kinhom',
                simulate=expression(
                  rpoispp(lambda))
                )
plot(Ken)
```

![](index_files/figure-html/unnamed-chunk-45-1.png)&lt;!-- --&gt;
]
]

---
class: inverse, bottom, right
# Covariates
---
## Covariates

.pull-left[
* an or multiple additional effects that influence the dependent variable other than the independent that we control for
* here might be elevation:
  * higher mountain ranges do probably have lesser population
  * also less archaeological investigation
  * We probably like to control for that 
]

.pull-right[
![](index_files/figure-html/unnamed-chunk-46-1.png)&lt;!-- --&gt;![](index_files/figure-html/unnamed-chunk-46-2.png)&lt;!-- --&gt;
.caption[above: Altitude of CZ. below: density of 14C dated sites]
]

---

### Elevation as Covariate

.pull-left[
Lets get the altitude data.

* raster data, we might like to use the raster package
* reprojection, since it comes as wgs84 lat/lng
* converting to `im` format, so that spatstat can understand it
]

.pull-right[
.tiny[

```r
library(raster)
library(maptools)

# Get Altitude Data (part of the raster package)
elev &lt;- raster::getData(name="alt", country = "CZ")

# project to UTM
elev.utm &lt;- projectRaster(elev, crs=32633)

# convert to im format
elev.im &lt;- as.im.RasterLayer(elev.utm)

plot(elev.im)
```
![](index_files/figure-html/unnamed-chunk-48-1.png)&lt;!-- --&gt;
]
]

---
## Nonparametric Estimate of Intensity as Function of a Covariate 

.pull-left[
We assume:
  * elevation and density (intensity) are correlated
   * we estimate and visualize the relationship using the function rhohat
  
The plot shows:
* lower altitudes have higher chances for dated sites
* above 400 m no significant effect change is visible

Only applicable for continuous values
]

.pull-right[
.tiny[

```r
rho &lt;- rhohat(data.ppp, elev.im,
              method="ratio")

plot(rho)
```

![](index_files/figure-html/unnamed-chunk-49-1.png)&lt;!-- --&gt;
]]

---

### Prediction from the 'model'

The relationship we identified represents a model.

Models can also be used generative: We can explore how the density would look like if our model (density depends on elevation) would be true

.pull-left[
.tiny[

```r
pred &lt;- predict(rho)
plot(pred, gamma = 0.25)
```

![](index_files/figure-html/unnamed-chunk-50-1.png)&lt;!-- --&gt;
]]

.pull-right[
.tiny[

```r
plot(data.kde, gamma = 0.25)
```

![](index_files/figure-html/unnamed-chunk-51-1.png)&lt;!-- --&gt;
]]

---
class: inverse, bottom, right
# Monte Carlo Simulation and Hypothesis tests

---

## Creating our own Monte Carlo procedure

Remember:


```r
ann.p &lt;- mean(nndist(data.ppp, k=1))
ann.p
```

```
## [1] 10613.68
```

If we want to simulate, we need:

* multiple instances of the process
* a process itself
* A loop to combine them

Lets simulate a random homogeneous point process:


```r
n     &lt;- 100               # Number of simulations
ann.r &lt;- vector(length = n) # Create an empty object to be used to store simulated ANN values
for (i in 1:n){
  rand.p   &lt;- rpoint(n=data.ppp$n, win=cz)  # Generate random point locations
  ann.r[i] &lt;- mean(nndist(rand.p, k=1))  # Collect the ANN values
}
```

---
### Results
.pull-left[
This is how one (the last) of the simulation results would look like:


```r
plot(rand.p)
```

![](index_files/figure-html/unnamed-chunk-54-1.png)&lt;!-- --&gt;

]

.pull-right[
We also can summarize using plots:
.tiny[

```r
hist(ann.r, xlim=range(ann.p, ann.r))
abline(v=ann.p, col="blue")
```

![](index_files/figure-html/unnamed-chunk-55-1.png)&lt;!-- --&gt;
Our actual ANN (blue line) is much smaller than the simulated distribution: Clustered.
]]
---

### based on elevation

.pull-left[
Now lets use the elevation as determining factor in the point process:


```r
plot(elev.im)
```

![](index_files/figure-html/unnamed-chunk-56-1.png)&lt;!-- --&gt;
]

.pull-right[
Because higher elevation means lesser probability, we have to invert the values. Also, we can rescale them to be between 0 and 1.

.tiny[

```r
elev.rescale &lt;- 1 - # invert
  (elev.im- min(elev.im)) /  # rescale between 0 and 1
  (max(elev.im)-min(elev.im))

plot(elev.rescale)
```

![](index_files/figure-html/unnamed-chunk-57-1.png)&lt;!-- --&gt;
]]

---
### Modeling based on Elevation

.pull-left[
Now we can use the same structure, but this time we provide f: the rescaled inverted elevation, as function for point probability:

.tiny[

```r
for (i in 1:n){
  rand.p   &lt;- rpoint(n=data.ppp$n, f=elev.rescale)
  ann.r[i] &lt;- mean(nndist(rand.p, k=1))
}
```
]]

.pull-right[
Histogram

```r
hist(ann.r, xlim=range(ann.p, ann.r))
abline(v=ann.p, col="blue")
```

![](index_files/figure-html/unnamed-chunk-59-1.png)&lt;!-- --&gt;
Also with this, the data remain clustered.
]

---
### Comparing  models

R has a conceptual framework with which models can be created and used for predictions, but also for comparison. This is always done in a standardised way. We first create a model in which we make the point density dependent on the height:

.pull-left[
.tiny[

```r
# Create the Poisson point process model
PPM1 &lt;- ppm(data.ppp ~ elev.im)
PPM1
```

```
## Nonstationary Poisson process
## 
## Log intensity:  ~elev.im
## 
## Fitted trend coefficients:
##   (Intercept)       elev.im 
## -17.815984604  -0.008123835 
## 
##                  Estimate        S.E.      CI95.lo       CI95.hi Ztest
## (Intercept) -17.815984604 0.322540945 -18.44815324 -17.183815968   ***
## elev.im      -0.008123835 0.001043406  -0.01016887  -0.006078797   ***
##                   Zval
## (Intercept) -55.236350
## elev.im      -7.785881
## Problem:
##  Values of the covariate 'elev.im' were NA or undefined at 0.66% (5 out of 754) 
## of the quadrature points
```
]]

.pull-right[
.tiny[

```r
# Plot the relationship
plot(effectfun(PPM1, "elev.im",
               se.fit=TRUE))
```

![](index_files/figure-html/unnamed-chunk-61-1.png)&lt;!-- --&gt;
]]

---
### Null model

We compare our model: 'elevation is a predictor for density' against the basic model (or null model) where we make the density not dependent on anything:

.pull-left[
.tiny[

```r
PPM0 &lt;- ppm(data.ppp ~ 1)
PPM0
```

```
## Stationary Poisson process
## Intensity: 1.003729e-09
##              Estimate      S.E.   CI95.lo   CI95.hi Ztest      Zval
## log(lambda) -20.71954 0.1125088 -20.94006 -20.49903   *** -184.1593
```
]]

.pull-right[
.tiny[

```r
# Plot the relationship
plot(effectfun(PPM0, "elev.im",
               se.fit=TRUE))
```

![](index_files/figure-html/unnamed-chunk-63-1.png)&lt;!-- --&gt;
]]

---
### Actual comparison

Often, anova is used to compare models. The `anova()` function will take the model objects as arguments, and return an ANOVA testing whether the more complex model is significantly better at capturing the data than the simpler model.

With our models, this results like this:

.tiny[

```r
anova(PPM0, PPM1, test="LRT")
```

```
## Warning: Values of the covariate 'elev.im' were NA or undefined at 0.66% (5
## out of 754) of the quadrature points. Occurred while executing: ppm.ppp(Q =
## data.ppp, trend = ~elev.im, data = NULL, interaction = NULL,
```

```
## Warning: Models were re-fitted after discarding quadrature points that were
## illegal under some of the models
```

```
## Analysis of Deviance Table
## 
## Model 1: ~1 	 Poisson
## Model 2: ~elev.im 	 Poisson
##   Npar Df Deviance  Pr(&gt;Chi)    
## 1    6                          
## 2    7  1   86.646 &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```
]

There is a significant increase in prediction accuracy compared to the null model!
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="libs/remark-latest.min.js"></script>
<script src="libs/macros.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"fig_caption": true,
"ratio": "16:10"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
